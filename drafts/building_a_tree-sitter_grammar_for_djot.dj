---toml
title = "Creating a Tree-sitter grammar for a subset of Djot"
tags = ["Djot", "Tree-sitter", "C"]
---

One of my favorite features in Neovim is the Tree-sitter integration.
It allows for fast syntax highlighting that works well even in an error state (often the case when you're editing code), and it has additional semantics (you can differentiate between function parameters and local variables).

With [nvim-treesitter-textobjects][] you can also jump between nodes (such as `]c` to jump to next class) or target deletion (`cif` to delete the function body and enter insert mode).
An amazing feature as it works across languages, no matter how they look like.

But, you might wonder how does Tree-sitter work?
How do you create a Tree-sitter parser for a language?

I started thinking about this and before I knew it I was trying to make my own parser for [Djot][] (a markup language similar to Markdown).
There are some good tutorials on how to get started, but not on some mare advanced things.

This is a decently long post that will go through:

1. How to use an external scanner
1. Conflicts resolution
1. Syntax highlighting, including language injection
1. Use the grammar from Neovim for highlighting and textobjects
1. Embed the grammar into this Blog with Rust

[nvim-treesitter-textobjects]: https://github.com/nvim-treesitter/nvim-treesitter-textobjects

# Our subset

For the purpose of this blog post, we'll implement a small subset of [Djot][]:

- Paragraphs
- Divs
- Code blocks
- Emphasis

This will allow us to parse markup like this:

````sdjot
This is a
multiline _paragraph_

:::
This is a paragraph inside a div
:::

```gleam
let x = 2;
```
````

(Yes, I [embedded the grammar into the Blog][Embedding the grammar with Rust] for the syntax highlighting.)

At first blush, this seems like it's too simple to require anything more complex other than some simple grammar rules, but later on we'll see that even these simple rules contain complicated edge-cases and gotchas.

# Simple beginnings

The point of this post isn't to go through how the Tree-sitter grammar description in `grammar.js` works.
The [Tree-sitter docs][] goes through how to get started pretty well.
I named the project `sdjot` and this is the `grammar.js` we'll start with:

[Tree-sitter docs]: https://tree-sitter.github.io/tree-sitter/creating-parsers

```javascript
module.exports = grammar({
  name: "sdjot",

  // Skip carriage returns.
  // We could skip spaces here as well, but the actual markup language
  // has significant spaces in some places, so let's remove them here too.
  extras: (_) => ["\r"],

  rules: {
    document: ($) => repeat($._block),

    // All blocks should end with a newline, but we can also parse multiple newlines.
    _block: ($) => choice($.div, $.code_block, $.paragraph, "\n"),

    // A div contains other blocks.
    div: ($) =>
      prec.left(seq($.div_marker, "\n", repeat($._block), $.div_marker, "\n")),
    div_marker: (_) => ":::",

    // Code blocks may have a language specifier.
    code_block: ($) =>
      seq(
        $.code_block_marker,
        optional($.language),
        "\n",
        $.code,
        $.code_block_marker
      ),
    code_block_marker: (_) => "```",
    code: (_) => repeat1(seq(/[^\n]*/, "\n")),
    language: (_) => /[^\s]+/,

    // A paragraph contains inline content and is terminated by a blankline
    // (two newlines in a row).
    paragraph: ($) => seq(repeat1(seq($._inline, "\n")), "\n"),

    // The markup parser could separate block and inline parsing into separate steps,
    // but we'll do everything in one parser.
    _inline: ($) => repeat1(choice($.emphasis, $._text)),
    emphasis: ($) => prec.left(seq("_", $._inline, "_")),
    _text: (_) => /[^\n]/,
  },
});
```

It recognizes paragraphs with text and emphasis, and it identifies divs and code blocks.

We can create an `example-file` with these contents:

```sdjot
:::
A paragraph _with emphasis_ inside a div

:::
```

And parse it with the `tree-sitter` cli:

```fish
$ tree-sitter parse example-file
(document [0, 0] - [5, 0]
  (div [0, 0] - [4, 0]
    (div_marker [0, 0] - [0, 3])
    (paragraph [1, 0] - [3, 0]
      (emphasis [1, 12] - [1, 27]))
    (div_marker [3, 0] - [3, 3])))
```

Et voilà!

## Missing features

But I told you it wasn't supposed to be this easy, and there are features missing from our parser.
Most notably:

1. There can be an arbitrary number of `:`, allowing divs to be nested.
1. Closing a div should close other open blocks (divs and paragraphs in our case).

In essence, we need to be able to parse this:

```sdjot
:::
Top-level div

::::
A paragraph inside a second div,
both closed when the top-level div is closed
:::
```

This is... Complicated.

Sure, we can work around the varying levels of `:` with something hacky like enumerating the number of colons, using something like this:

```javascript
div: ($) => choice($._div3, $._div4, $._div5, $._div6, $._div7, $._div8),
_div3: ($) => seq(/:{3}/, $._inside_div, /:{3}/, "\n"),
_div4: ($) => seq(/:{4}/, $._inside_div, /:{4}/, "\n"),
_div5: ($) => seq(/:{5}/, $._inside_div, /:{5}/, "\n"),
_div6: ($) => seq(/:{6}/, $._inside_div, /:{6}/, "\n"),
_div7: ($) => seq(/:{7}/, $._inside_div, /:{7}/, "\n"),
_div8: ($) => seq(/:{8}/, $._inside_div, /:{8}/, "\n"),
_inside_div: ($) => prec.left("\n", repeat($._block)),
```

But it's not _neat_, and automatically closing matching blocks is much harder (to my brain it seems impossible, but I'm no expert).

With an external scanner we can do this (and more).

# External scanner

A Tree-sitter parser is actually a C program.
The grammar we've seen has been described in JavaScript, but it's only used as a description to generate the parser in C.
If you're a masochist, you can take a look at it in `src/parser.c` after running `tree-sitter generate`.

An external scanner is just some custom C code that's inserted into the parser, and it allows us to override the parser precedence, keep track of a context state, or whatever else we might need or want to do.

To get started the [official docs][external-scanners] was pretty good.
Basically you need to:

1. Create a `src/scanner.c` and include it in `binding.gyp` `bindings/rust/build.rs`.
1. Setup `externals` tokens in `grammar.js` and a matching C enum in `scanner.c`.
1. Define and implement five C functions.

Let's take a look.

## Div markers closes open paragraphs

Let's start by closing a paragraph early when a `:::` is encountered.
This is simpler because we don't have to store any state.

When parsing `$.paragraph` we'll give the parser a choice between ending the paragraph on a newline or on our new `$._close_paragraph` token:


```javascript
paragraph: ($) =>
  seq(repeat1(seq($._inline, "\n")), choice("\n", $._close_paragraph)),
```

`$._close_paragraph` is handled by the external scanner, which is specified using the `externals` field:

```javascript
externals: ($) => [$._close_paragraph],
```

Now let's turn our attention to `src/scanner.c`.

The tokens in `externals` gets assigned an incremented number, starting from 0...
Just like an enum in C!

```c
// We only have a single element right now, but keep in mind that the order
// must match the `externals` array in `grammar.js`.
typedef enum { CLOSE_PARAGRAPH } TokenType;
```

The five functions we need to implement are these:

```c
// You should replace `sdjot` with whatever project name you chose.
bool tree_sitter_sdjot_external_scanner_scan(void *payload, TSLexer *lexer,
                                             const bool *valid_symbols) {
  // All the scanning goes here.
  return false;
}

// If we need to allocate/deallocate state, we do it in these functions.
void *tree_sitter_sdjot_external_scanner_create() { return NULL; }
void tree_sitter_sdjot_external_scanner_destroy(void *payload) {}

// If we have state, we should load and save it in these functions.
unsigned tree_sitter_sdjot_external_scanner_serialize(void *payload,
                                                      char *buffer) {
  return 0;
}
void tree_sitter_sdjot_external_scanner_deserialize(void *payload, char *buffer,
                                                    unsigned length) {}
```

Because we won't use any state, we'll only have to update the `scan` function.

What you're supposed to do is check `valid_symbols` for the tokens we can return at any point in time, and return `true` if any was found:

```c
bool tree_sitter_sdjot_external_scanner_scan(void *payload, TSLexer *lexer,
                                             const bool *valid_symbols) {
  if (valid_symbols[CLOSE_PARAGRAPH] && parse_close_paragraph(lexer)) {
    return true;
  }
  return false;
}
```

The matched token should be stored in `lexer->result_symbol`c:

```c
bool parse_close_paragraph(TSLexer *lexer) {
  // Mark the end before advancing so that the CLOSE_PARAGRAPH token doesn't
  // consume any characters.
  lexer->mark_end(lexer);

  uint8_t colons = consume_chars(lexer, ':');
  if (colons == 3) {
    lexer->result_symbol = CLOSE_PARAGRAPH;
    return true;
  } else {
    return false;
  }
}
```

Note that the resulting token will mark any symbol we advance over as owned by that token.
So `:::` would be marked as `_close_paragraph` (which will be ignored by the output since it begins with an underscore), instead of `div_marker`.
To stop prevent this, we turn `_close_paragraph` into a zero-width token by marking the end before advancing the lexer.

How do we advance the lexer?
We call `lexer->advance`c:

```c
uint8_t consume_chars(TSLexer *lexer, char c) {
  uint8_t count = 0;
  while (lexer->lookahead == c) {
    lexer->advance(lexer, false);
    ++count;
  }
  return count;
}
```

This is almost all we can do with the lexer.
We only process one character at a time, cannot look behind, and our only tool to look ahead is to `mark_end` at the correct place.
(We can also query the current column position.)

With this we have a working external scanner and div tags now close paragraphs:

```sdjot
:::
A paragraph inside a div
:::
```

```fish
$ tree-sitter parse example-file
(document [0, 0] - [4, 0]
  (div [0, 0] - [3, 0]
    (div_marker [0, 0] - [0, 3])
    (paragraph [1, 0] - [2, 0])
    (div_marker [2, 0] - [2, 3])))
```

(This may not be the best design, having to parse div markers multiple times, but it gets the job done.)

## Nested blocks

To automatically close other open blocks we need to add some context to our parser, which means we'll need state management.

The small subset we're using for the blog is only concerned with divs because it would be a terribly long post otherwise, but I'll try to implement this in a general manner, to be more indicative of a real-world parser.

Our strategy is this:

1. A div can have a varying number of `:` that must match.

   Therefore we'll parse colons in an external scanner and store it on a stack.

1. When we find a div marker, we'll need to decide if it should start a new div, or close an existing one.

   We'll look at the stack of open blocks and see if we find a match.

1. If we have need to close a nested div, that is if we want to close a div further down the stack, we need to close the nested div(s) first.

   Thus we'll introduce a `block_close` marker that ends a div, and leave the ending div marker as optional.

First we'll ask the grammar to let the external scanner manage the begin and end tokens.
We'll use a `_block_close` marker to end the div, and leave the end marker optional.
(You could probably use a `choice()` between the two, but this made more sense to me when I was implementing it.)

```javascript
div: ($) =>
  prec.left(
    seq(
      // A rule starting with "_" will be hidden in the output,
      // and we can use "alias" to rename rules.
      alias($._div_marker_begin, $.div_marker),
      "\n",
      repeat($._block),
      $._block_close,
      optional(alias($._div_marker_end, $.div_marker))
    )
  ),

externals: ($) => [
  $._close_paragraph,
  $._block_close,
  $._div_marker_begin,
  $._div_marker_end,

  // This is used in the scanner internally,
  // but shouldn't be used by the grammar.
  $._ignored,
],
```

And remember to update the list of external tokens in the scanner (order matters):

```c
typedef enum {
  CLOSE_PARAGRAPH,
  BLOCK_CLOSE,
  DIV_MARKER_BEGIN,
  DIV_MARKER_END,
  IGNORED
} TokenType;
```

Then to our stack of blocks.

I used a `Block` type to keep track of the type and number of colons:

```c
// In a real implementation we'll have more block types.
typedef enum { DIV } BlockType;

typedef struct {
  BlockType type;
  uint8_t level;
} Block;
```

I know that `level` isn't the best name, but I couldn't find a very good general names for the number of colons, indentation level, etc.
With sum types you could model it in a clearer way, like this:

```rust
enum Block {
    Div { colons: u32 },
    Footnote { indent: u32 },
    // etc
}
```

But I digress, this is good enough.

> I will, in fact, claim that the difference between a bad programmer and a good one
> is whether he considers his code or his data structures more important.
> Bad programmers worry about the code.
> Good programmers worry about data structures and their relationships.
> ^ Linus Torvalds

Another joy of programming C is that you'll get to re-implement standard data structures such as a growable stack.
It's not truly difficult, but it's annoying and bug-prone.

Luckily, during the time I'm writing this blog post, [tree-sitter 0.22.1][] was released with an array implementation.
So now I don't have to show you my shoddy stack implementation, and we can use their array for our stack instead.

We'll shove our `Array` of `Block*` into a `Scanner` struct, because we'll need to track more data later:

```c
#include "tree_sitter/array.h"

typedef struct {
  Array(Block *) * open_blocks;
} Scanner;
```

When you manage state in tree-sitter, you need to do some data management in the `tree_sitter_` functions we defined earlier.

Allocations are managed in the `_create` and `_destroy` functions.
Also new for 0.22.1 is the recommendation to use `ts_` functions for allocations, to allow consumers to override the default allocator:

```c
#include "tree_sitter/alloc.h"

void *tree_sitter_sdjot_external_scanner_create() {
  Scanner *s = (Scanner *)ts_malloc(sizeof(Scanner));

  // This is how you create an empty array
  s->open_blocks = ts_malloc(sizeof(Array(Block *)));
  array_init(s->open_blocks);

  return s;
}

void tree_sitter_sdjot_external_scanner_destroy(void *payload) {
  Scanner *s = (Scanner *)payload;

  // I haven't shown the allocation of the blocks yet,
  // but keep in mind that `array_delete` does not deallocate any memory
  // you store in the array itself.
  for (size_t i = 0; i < s->open_blocks->size; ++i) {
    // `array_get` is preferrable even though you can index the contents directly.
    ts_free(array_get(s->open_blocks, i));
  }

  // The array is a growable one, `array_delete` ensures that the
  // memory is deleted.
  array_delete(s->open_blocks);

  ts_free(s);
}
```

I allocate the blocks in a `push_block` helper:

```c
static void push_block(Scanner *s, BlockType type, uint8_t level) {
  Block *b = ts_malloc(sizeof(Block));
  b->type = type;
  b->level = level;

  array_push(s->open_blocks, b);
}
```

You also need to define the serialize functions.
These store and retrieve the managed state, to allow tree-sitter to backtrack.

```c
unsigned tree_sitter_sdjot_external_scanner_serialize(void *payload,
                                                      char *buffer) {
  Scanner *s = (Scanner *)payload;
  unsigned size = 0;
  for (size_t i = 0; i < s->open_blocks->size; ++i) {
    Block *b = *array_get(s->open_blocks, i);
    buffer[size++] = (char)b->type;
    buffer[size++] = (char)b->level;
  }
  return size;
}

void tree_sitter_sdjot_external_scanner_deserialize(void *payload, char *buffer,
                                                    unsigned length) {
  Scanner *s = (Scanner *)payload;
  array_init(s->open_blocks);
  size_t size = 0;
  while (size < length) {
    BlockType type = (BlockType)buffer[size++];
    uint8_t level = (uint8_t)buffer[size++];
    push_block(s, type, level);
  }
}
```

And that's the (initial) state management taken care of!

::: notice
C is scary.
Try [valgrind][].
:::


## Div markers

Of course, we haven't used our state yet.
Let's change that.

First, let's add the `parse_div` entry point to our scan function:

```c
bool tree_sitter_sdjot_external_scanner_scan(void *payload, TSLexer *lexer,
                                             const bool *valid_symbols) {
  Scanner *s = (Scanner *)payload;

  // Paragraph needs to be closed before we try to close divs.
  if (valid_symbols[CLOSE_PARAGRAPH] && parse_close_paragraph(lexer)) {
    return true;
  }

  // Check `valid_symbols` inside `parse_div` because of multiple valid symbols.
  if (parse_div(s, lexer, valid_symbols)) {
    return true;
  }

  return false;
}
```

Because advancing the lexer is primitive, and we cannot "go back a char", it's important to only advance it if we really need to.
Therefore we always need to check `valid_symbols` before we continue:

```c
bool parse_div(Scanner *s, TSLexer *lexer, const bool *valid_symbols) {
  if (!valid_symbols[DIV_MARKER_BEGIN] && !valid_symbols[DIV_MARKER_END]) {
    return false;
  }

  // ...
}
```

Next we'll need to consume all colons we're at, and only continue if we see at least three:

```c
uint8_t consume_chars(TSLexer *lexer, char c) {
  uint8_t count = 0;
  while (lexer->lookahead == c) {
    lexer->advance(lexer, false);
    ++count;
  }
  return count;
}

bool parse_div(Scanner *s, TSLexer *lexer, const bool *valid_symbols) {
  // ...

  uint8_t colons = consume_chars(lexer, ':');
  if (colons < 3) {
    return false;
  }

  // ...
}
```

Opening a new div is simple; we push the block and register the number of colons:

```c
push_block(s, DIV, colons);
lexer->result_symbol = DIV_MARKER_BEGIN;
return true;
```

But to the decide if we should open or close a div, we need a way to search through the stack.
This function does that, while also returning how many blocks deep into the stack we found the div (which will use shortly):

```c
// How many blocks from the top of the stack can we find a matching block?
// If it's directly on the top, returns 1.
// If it cannot be found, returns 0.
static size_t number_of_blocks_from_top(Scanner *s, BlockType type,
                                        uint8_t level) {
  for (int i = s->open_blocks->size - 1; i >= 0; --i) {
    Block *b = *array_get(s->open_blocks, i);
    if (b->type == type && b->level == level) {
      return s->open_blocks->size - i;
    }
  }
  return 0;
}

bool parse_div(Scanner *s, TSLexer *lexer, const bool *valid_symbols) {
  // ...

  size_t from_top = number_of_blocks_from_top(s, DIV, colons);

  // We could check if either DIV_MARKER_BEGIN or DIV_MARKER_END are valid here,
  // but as the grammar is set up they're both always valid at the same time.
  if (from_top > 0) {
    // Close the current div, and all blocks above.
  } else {
    // No matching div to close, let's open a new.
    lexer->mark_end(lexer);
    push_block(s, DIV, colons);
    lexer->result_symbol = DIV_MARKER_BEGIN;
    return true;
  }
}
```

But we have a problem: when we want to close the div, we want to be able to output multiple tokens.

For example, with this type of input:

```sdjot
:::
::::
:::::::
inception
:::
```

We have a stack of 3 divs:

```
7 (top)
4
3 (the one we want to close)
```

In the code above, `from_top` will be `3` and we need to output 4 tokens: 3 `BLOCK_CLOSE` (one for each div) and 1 `DIV_MARKER_END` (for the last `:::`).
But the scanner can only output a single token at a time.

The way I solved this is by introducing more state to the Scanner.
Specifically, I introduced a `blocks_to_close` variable that we'll use to output `BLOCK_CLOSE`, and some variables to output (and consume) the `DIV_MARKER_END`.

```c
typedef struct {
  Array(Block *) * open_blocks;

  // How many BLOCK_CLOSE we should output right now?
  uint8_t blocks_to_close;

  // Delayed output of a token.
  TokenType delayed_token;
  // Allows us to consume the width of a delayed token.
  uint8_t delayed_token_width;
} Scanner;
```

We need to remember to update the create and serialize functions too.

Serialize:

```c
buffer[size++] = (char)s->blocks_to_close;
buffer[size++] = (char)s->delayed_token;
buffer[size++] = (char)s->delayed_token_width;
```

Deserialize:

```c
s->blocks_to_close = (uint8_t)buffer[size++];
s->delayed_token = (TokenType)buffer[size++];
s->delayed_token_width = (uint8_t)buffer[size++];
```

We'll use `IGNORED` as the unused token, so we'll need to reset it when we create the scanner:

```c
s->blocks_to_close = 0;
s->delayed_token = IGNORED;
```

Now when we scan we should first check `blocks_to_close` and then `delayed_token`, before we scan other things:

```c
bool tree_sitter_sdjot_external_scanner_scan(void *payload, TSLexer *lexer,
                                             const bool *valid_symbols) {
  Scanner *s = (Scanner *)payload;

  if (valid_symbols[BLOCK_CLOSE] && handle_blocks_to_close(s, lexer)) {
    return true;
  }

  if (output_delayed_token(s, lexer, valid_symbols)) {
    return true;
  }

  // Parse the other stuff
}
```

When we see `blocks_to_close > 0`, then we should output a `BLOCK_CLOSE` and remove the top block (with some sanity checks for good measure):

```c
void remove_block(Scanner *s) {
  if (s->open_blocks->size > 0) {
    ts_free(array_pop(s->open_blocks));
    if (s->blocks_to_close > 0) {
      --s->blocks_to_close;
    }
  }
}

bool handle_blocks_to_close(Scanner *s, TSLexer *lexer) {
  if (s->open_blocks->size == 0) {
    return false;
  }

  // If we reach eof with open blocks, we should close them all.
  if (lexer->eof(lexer) || s->blocks_to_close > 0) {
    lexer->result_symbol = BLOCK_CLOSE;
    remove_block(s);
    return true;
  }
  return false;
}
```

With this we can output multiple `BLOCK_CLOSE`, and now to handle delayed tokens:

```c
bool output_delayed_token(Scanner *s, TSLexer *lexer,
                          const bool *valid_symbols) {
  if (s->delayed_token == IGNORED || !valid_symbols[s->delayed_token]) {
    return false;
  }

  lexer->result_symbol = s->delayed_token;
  s->delayed_token = IGNORED;
  // With `delayed_token_width` we can consume the ending `:::`, for example.
  while (s->delayed_token_width--) {
    lexer->advance(lexer, false);
  }
  lexer->mark_end(lexer);
  return true;
}
```

Another way to design this is to have a stack of delayed tokens and then just pop that.
It's certainly more powerful, I just happened to choose this way when I was playing around with it.

Either way, we can now implement the div end handling. In `parse_div`:

```c
size_t from_top = number_of_blocks_from_top(s, DIV, colons);

if (from_top > 0) {
  // Found a div we should close.
  close_blocks_with_final_token(s, lexer, from_top, DIV_MARKER_END, colons);
  return true;
} else {
  lexer->mark_end(lexer);
  push_block(s, DIV, colons);
  lexer->result_symbol = DIV_MARKER_BEGIN;
  return true;
}
```

`close_blocks_with_final_token` is a general helper that sets up the number of blocks to close and the final token:

```c
void close_blocks_with_final_token(Scanner *s, TSLexer *lexer, size_t count,
                                   TokenType final, uint8_t final_token_width) {
  remove_block(s);
  s->blocks_to_close = s->blocks_to_close + count - 1;
  lexer->result_symbol = BLOCK_CLOSE;
  s->delayed_token = final;
  s->delayed_token_width = final_token_width;
}
```

Now we can finally try to close divs:

```sdjot
::::
:::
:::::::
Divception
:::

```

```fish
$ tree-sitter parse example-file
(document [0, 0] - [6, 0]
  (div [0, 0] - [6, 0]
    (div_marker [0, 0] - [0, 4])
    (div [1, 0] - [4, 3]
      (div_marker [1, 0] - [1, 3])
      (div [2, 0] - [4, 0]
        (div_marker [2, 0] - [2, 7])
        (paragraph [3, 0] - [4, 0]))
      (div_marker [4, 0] - [4, 3]))))
```

We can see that it parses without error, the last marker closes the second div correctly, and the last marker captures the final `:::`.


# Handling conflicts

Our grammar works pretty well, but there are issues you might want to fix.
One issue, that took much longer to figure out than I care to admit, is adding a fallback to text when a markup rule doesn't match.

A simple example for our grammar is a single underscore in a paragraph:

```
a_b
```

I'd assume this would produce a paragraph with text, but instead we'll get an error:

```fish
$ tree-sitter parse example-file
(document [0, 0] - [2, 0]
  (ERROR [0, 0] - [0, 3]))
```

This is weird, because one of the main selling points of Tree-sitter is the GLR algorithm, which should explore the different interpretations to find something that succeeds.
But for some reason, it doesn't trigger for us.

Let's take a look.
These are the relevant lines from the grammar:

```javascript
_inline: ($) => repeat1(choice($.emphasis, $._text)),
emphasis: ($) => prec.left(seq("_", $._inline, "_")),
_text: (_) => /[^\n]/,
```

When we try to match a `_` then the grammar can match either `emphasis` or `_text` because `_` matches both `"_"`javascript and `/[^\n]/`javascript.
The issue seems to be that Tree-sitter doesn't recognize this as a conflict.

If we instead add a fallback with a `_` string then Tree-sitter will treat it as a conflict:

```javascript
_inline: ($) => repeat1(choice($.emphasis, $._text, $._fallback)),
emphasis: ($) => prec.left(seq("_", $._inline, "_")),
_fallback: (_) => "_",
_text: (_) => /[^\n]/,
```

And when we call `tree-sitter generate` we're made aware of the conflict:

```
$ tree-sitter generate
Unresolved conflict for symbol sequence:

  '_'  •  '_'  …

Possible interpretations:

  1:  (_fallback  '_')  •  '_'  …
  2:  (emphasis  '_'  •  _inline  '_')  (precedence: 0, associativity: Left)

Possible resolutions:

  1:  Specify a higher precedence in `emphasis` than in the other rules.
  2:  Specify a higher precedence in `_fallback` than in the other rules.
  3:  Specify a left or right associativity in `_fallback`
  4:  Add a conflict for these rules: `emphasis`, `_fallback`
```

What we want to do is mark them as a conflict that's supposed to exist in the grammar using the `conflicts` field:

```javascript
conflicts: ($) => [[$.emphasis, $._fallback]],
```

And now we can parse paragraphs containing only a single `_` without errors.

I'm not 100% sure, but it doesn't seem like you can trigger the GLR algorithm with a token returned by an external scanner.


# Some tests

Using `tree-sitter parse example-file` (with or without the `-d` or `-D` flags, try them if you haven't) is fine for experimental tests, but we really should add the different test cases as proper unit tests.
Tree-sitter has a built-in test harness for this purpose.

Let's add the very first test case to `test/corpus/syntax.txt`:

````
===============================================================================
Parsing goal
===============================================================================
This is a
multiline _paragraph_

:::
This is a paragraph inside a div
:::

```gleam
let x = 2;
```

-------------------------------------------------------------------------------

(document
  (paragraph (emphasis))
  (div
    (div_marker)
    (paragraph)
    (div_marker))
  (code_block
    (code_block_marker)
    (language)
    (code)
    (code_block_marker)))
````

And run it:

```
$ tree-sitter test
  syntax:
    ✓ Parsing goal
```

Yay!

We should add (a lot) more tests here, but I won't bother writing them out in this already too long blog post.


# Using tree-sitter for something useful

## Syntax highlighting

Syntax highlighting is made using queries from the `highlights.scm` file.
It's common to have it placed in the `src` directory in the same repository as the grammar, but it's not required.

Here's an example `src/highlights.scm` file that highlights the different elements of our markup:

```scm
(div_marker) @punctuation.delimiter
(code_block_marker) @punctuation.delimiter

(emphasis "_" @punctuation.delimiter) @markup.italic
(language) @tag.attribute

(code_block) @markup.raw
(paragraph) @markup
```

What colors to choose is a bit arbitrary, these works well enough I suppose.

See the [documentation][syntax-highlight] for more details on how the queries and highlighting works.

[syntax-highlight]: https://tree-sitter.github.io/tree-sitter/syntax-highlighting

## Language injection

One big question I had when starting writing my grammar was how to mix multiple parsers in the same document, to for example highlight code blocks using the specified language:

````sdjot
```gleam
let x = 2;
```
````

Turns out, this is quite straightforward.

With the initial grammar, the code block parses into:

```
(code_block
  (code_block_marker)
  (language)
  (code)
  (code_block_marker)))
```

Which we'll use in `src/injections.scm` to specify that we want to parse `(code)` using the grammar specified in `(language)`:

```scm
(code_block
  (language) @injection.language
  (code) @injection.content)
```


## Using our grammar with Neovim

I typically install Tree-sitter grammars in Neovim using `:TSInstall` provided by [nvim-treesitter][].
But you can [install local Tree-sitter grammars][] as well:

```lua
local parser_config = require("nvim-treesitter.parsers").get_parser_configs()
parser_config.sdjot = {
    install_info = {
        -- Change this url to your grammar
        url = "~/code/tree-sitter-sdjot",
        -- If you use an external scanner it needs to be included here
        files = { "src/parser.c", "src/scanner.c" },
        generate_reqires_npm = false,
        requires_generate_from_grammar = false,
    },
    -- The filetype you want it registered as
    filetype = "sdjot",
}
```

Just make sure you have a `"tree-sitter"` section in the grammar's `package.json`:

```json
"tree-sitter": [
  {
    "scope": "source.sdjot",
    "file-types": [
      "sdj"
    ],
    "injection-regex": "sdjot"
  }
],
```

With this you can do `:TSInstall sjdot` and `:TSUdate sdjot` when you make changes.

`:TSInstall` doesn't install queries automatically though.
What I did was to just symlink the queries directory into Neovims config directory:

```fish
ln -s ~/code/tree-sitter-sdjot/queries ~/.config/nvim/queries/sdjot
```

`:TSPlaygroundToggle` is very useful for debugging the grammar, and `:Inspect` shows you the highlight groups under your cursor.
It might be good to check out `:help treesitter-highlight-groups` if you want to play with your theme.


## Jumping and selecting with textobjects

I mentioned [nvim-treesitter-textobjects][] as a good example of why Tree-sitter is about more than syntax highlighting.

To make use of our grammar we can add some capture groups to `src/textobjects.scm`.
For example we can register our code blocks as "functions":

```scm
(code_block (code) @function.inner) @function.outer
```

The objects are arbitrary, but `@function` is one of the standard objects so I guess it might make sense.

With the symlink ready, you need to register keymaps with [nvim-treesitter-textobjects][] and you're good to go.
I have it setup so I can jump between `@function.outer` with `[f` and `]f`, and selections with `af` and `if`.

This means that with the above textobject definition I can for example jump to the next code block with `]f` and then remove all the code inside with `cif` to end up in insert mode, ready to replace it with some new code.

## Embedding the grammar with Rust

```toml
[dependencies]
tree-sitter-highlight = "^0.20.0"
tree-sitter-sdjot = { git = "https://github.com/treeman/tree-sitter-sdjot.git" }
```

```rust
// All highlights needs to be listed explicitly.
static HIGHLIGHT_NAMES: &[&str] = &[
    // I have +100 entries here, this is for sdjot.
   "markup",
   "markup.italic",
   "markup.raw",
   "punctuation.delimiter",
   "tag.attribute",
];

lazy_static! {
    static ref CONFIGS: HashMap<String, HighlightConfiguration> = init_configurations();
}

fn init_configurations() -> HashMap<String, HighlightConfiguration> {
    [
        // I have more languages here
        (
            "sdjot",
            HighlightConfiguration::new(
                tree_sitter_sdjot::language(),
                tree_sitter_sdjot::HIGHLIGHTS_QUERY,
                tree_sitter_sdjot::INJECTIONS_QUERY,
                "",
            )
            .unwrap(),
        ),
    ]
    .into_iter()
    .map(|(name, mut config)| {
        config.configure(&HIGHLIGHT_NAMES);
        (name.to_string(), config)
    })
    .collect()
}
```

```fish
rg "@[\w.]+" -INo --trim highlights.scm | sort | uniq
```

```rust
pub struct TreesitterHighlighter<'a> {
    config: &'a HighlightConfiguration,
}

impl<'a> TreesitterHighlighter<'a> {
    pub fn find(lang_id: &str) -> Option<Self> {
        CONFIGS.get(lang_id).map(|config| Self { config })
    }
}
```

```rust
impl<'a> TreesitterHighlighter<'a> {
    pub fn highlight(&self, code: &str) -> Result<String> {
        let mut highlighter = Highlighter::new();

        let highlights = highlighter
            .highlight(self.config, code.as_bytes(), None, |lang| {
                let res = CONFIGS.get(lang);
                if !res.is_some() {
                    warn!("Couldn't find treesitter grammar for `{lang}` to inject");
                }
                res
            })
            .unwrap();

        let mut renderer = HtmlRenderer::new();
        renderer.render(highlights, code.as_bytes(), &|attr| {
            // This isn't very nice... How to generate strings dynamically from inside a Fn closure
            // that returns a byte slice?
            // Not easily.
        })?;
        let res = renderer.lines().join("");
        Ok(res)
    }
}
```

```rust
lazy_static! {
    static ref CLASSES: Vec<String> = HIGHLIGHT_NAMES
        .iter()
        .map(|name| format!(r#"class="{}""#, name.replace(".", " ")))
        .collect();
}
```

```rust
renderer.render(highlights, code.as_bytes(), &|attr| {
    CLASSES[attr.0].as_bytes()
})?;
```

Very odd problems. For instance, if I used `$.code`javascript to capture the embedded code, it wouldn't work. But with `optional($.code)`javascript, it.

[tree_sitter_highlight]: https://docs.rs/tree-sitter-highlight/latest/tree_sitter_highlight/

# Within edge-cases lies complexity

> The night is dark and full of terrors
> ^ George R.R. Martin

If you've read the post to the end, congratulations.
You made it!

I don't claim to be an expert at grammars or Tree-sitter, and I'm sure there are plenty of things that can be improved with the way the grammar is made.
But I hope it can be helpful as a starting point if you're curious on how to write some Tree-sitter grammar of your own.

Just one word of advice before you go.
Writing a grammar for simple rules is pretty easy, but in the real world things can get messy quickly.
This is especially true if you need to juggle multiple conflicting rules in the external scanner---keeping a sane structure is challenging.

[tree-sitter 0.22.1]: https://github.com/tree-sitter/tree-sitter/releases/tag/v0.22.1
[external-scanners]: https://tree-sitter.github.io/tree-sitter/creating-parsers#external-scanners
[valgrind]: https://valgrind.org/
[Djot]: https://djot.net/
[nvim-treesitter]: https://github.com/nvim-treesitter/nvim-treesitter
[install local Tree-sitter grammars]: https://github.com/nvim-treesitter/nvim-treesitter#adding-parsers

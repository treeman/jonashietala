---toml
title = "Kubernetes"
tags = ["Some tag"]
---

{=Building a Kubernetes cluster for my Homelab=}

<https://iamsafts.com/posts/homelab-intro/>

Price wise it probably would've been a smarter choice to play around with Kubernetes in the Proxmox server I already have running, or maybe to base it on a few mini-PCs.
But I figured that I wanted to try out Kubernetes for _real_ with a bunch of real computers and I

I decided to build my Kubernetes cluster using six Raspberry Pi 5s.

{=Preparing the Raspberry Pis=}

# Hardware

![My 6 Raspberry Pis](/images/kubernetes/pi_hanging.jpg)

It's probably disappointing to hear that I didn't put a ton of thought into the hardware as I maybe should've.
I emailed a little with Sergios Aftsidis who [did a more serious analysis](https://iamsafts.com/posts/rpi_k8s/part1_hardware/) and he recommended me to consider 1TB NVMe drives over 500GB for Longhorn, and at least the 8 GB versions of the Raspberry Pi 5.

That sounded good to me and I ended up with six 8GB Pi 5 and six [PNY CS2230 M.2 NVMe SSD 1TB][] (because I wanted to buy from Inet and they seemed like good options).

If the hardware ends up limiting me I can always add more powerful nodes to the cluster in the future.

::: note
With just the Pis I can't really move my [Jellyfin][] service to the cluster as they don't have a good enough GPU for the transcoding needs I have.
I might spin up a node in my Proxmox server for that purpose but I haven't decided yet.
:::

[Jellyfin]: https://jellyfin.org/

# Building a mini rack

![My cute computers are hanging in a 3D printed mini rack.](/images/kubernetes/kube_rack_front.jpg)

What actually sparked this side-quest of building a Kubernetes setup for my Homelab was watching Jeff Geerling's video [about the MINI RACK][].
It's a totally silly reason but I like building neat looking things and a [MINI RACK][] would be a perfect way of cleaning up my Homelab devices scattered around the house.

Except that I built a rack for new devices... But that's a tale for another day.

I went searching and sure enough you [can 3D print the entire rack][3d-printed-rack]!
Including the screws! (Although you have to be a little nuts to do that. Or lazy.)

To mount the Pis I used the [Server Mark III for Raspbery Pi 5][] model.
They didn't fit the 10" rack perfectly and with the setup I went for I could squeeze a Raspberry Pi 4 there (mostly for looks).
It's too tight to add a 7th Pi with an NVMe drive so I'm not really sure what to do with it but for now I'll let it hang around.


TODO images on the power supply for the Pis


# Breathing life into the Pis

Before installing Kubernetes you have to install an Operating System to run it on.
I also had to install it onto the NVMe card and do it without having to attach a screen or something.

I was worried it would be very complicated but it was pretty simple.

## What OS to run?

Decisions, decisions.\
What operating system to choose?\
Should I go with [Void Linux][], the OS I run on my own machines?\
Maybe a [Raspberry Pi OS][] that should already be configured properly for the device?\
[Ubuntu server][] because a lot of tutorials I reference use it?\
Or even [Talos Linux][] which is a pre-packaged OS for Kubernetes?

While I really like the idea of [Talos Linux][] Raspberry Pi 5 isn't officially supported and it takes over the when NVMe drive, while I wanted to separate out a storage partition for Longhorn and local-path storage.

So I took the easy route of choosing [Ubuntu server][] this time.
I don't think the OS matters that much; all it will do is run Kubernetes anyway.

## Install Ubuntu on NVMe drive

![An USB to NVMe device may save you a lot of time.](/images/kubernetes/nvme_usb.jpg)

If you can connect NVMe drives via USB (they're really cheap) the installation process is straightforward:

1. Flash Ubuntu Server LTS using `rpi-imager` on NVMe drive.

   Make sure to update the hostname and ssh credentials.

   I chose the hostnames  `kube-pi5a`, `kube-pi5a`, etc for the IP addresses `10.1.2.2`, `10.1.2.3`, etc. (`10.1.2.1` is reserved for the virtual IP for the whole cluster).

1. Add another partition using `gparted` (or similar).

   I first resized the main partition to 128 GB and then I added a new partition for the persistent storage.

1. Insert the NVMe drive into the Pi and you should be able to `ssh` into it.

::: note
There are lots of guides detailing additional steps to enable booting from the NVMe drive but I personally didn't have to do them.
:::

## Prepare for Kubernetes

We're not quite ready to install Kubernetes just yet.
There's some more setup we need to do before we can jump into the installation.

1. Assign static IP

   Kubernetes requires you to assign static IP addresses to all nodes.
   I use [OPNsense][] and I simply added a DHCP static mapping for each device.

1. Allow user to run `sudo` without password prompt

   (This is needed for `k3sup` later on.)

   {path="/etc/sudoers"}
   ```
   your-username ALL=(ALL) NOPASSWD:ALL
   ```

1. Update packages

   Maybe not required but I think it's good form to try to stay up-to-date.

   ```
   sudo apt update; sudo apt upgrade
   ```

1. Edit `/etc/fstab` to mount the new data partition to `/mnt/data`:

   {path="/etc/fstab"}
   ```
   LABEL=system-boot       /boot/firmware  vfat    defaults        0       1
   LABEL=writable          /               ext4    defaults        0       1
   /dev/nvme0n1p3          /mnt/data       ext4    defaults        0       1
   ```

   (Maybe I should've used a label for the data partition too but I forgot to add it.)

1. [Add `cgroup` flags][part2_bootstrap]

   Edit `/boot/firmware/cmdline.txt` and add:

   {path="/boot/firmware/cmdline.txt"}
   ```
   cgroup_enable=cpuset cgroup_enable=memory cgroup_memory=1
   ```

1. [Force Gen 3.0 speeds][part2_bootstrap] for the NVMe drives

   Edit `/boot/firmware/config.txt` and add `dtparam` like so:

   {path="/boot/firmware/config.txt"}
   ```
   [all]
   # Force PCIe Gen 3.0 speeds
   dtparam=pciex1_gen=3
   ```

Just reboot and do this 5 times more :)

You can write a script to do it but I did it manually, it wasn't _that_ bad.

[part2_bootstrap]: https://iamsafts.com/posts/rpi_k8s/part2_bootstrap/

---

# What Kubernetes distribution to choose?

One of the things that contributes to perceived complexity of Kubernetes is the amount of choices you have.
Following that theme there's a bunch of distributions you can install; [k3s][], [minikube][], [RKE2][], [Talos Linux][], etc etc.

After looking around a bit in the end my choice was between [k3s][] and [RKE2][].
[k3s][] is the more lightweight distribution and has more tutorials, while [RKE2][] seems like the better choice security wise.
I was going to go with [RKE2][] but I was a bit worried that it wasn't as lightweight as [k3s][] and it was a little harder to find guides for it, so I ended up with [k3s][].

I'm a little bummed out that I couldn't use [Talos Linux][] but on the other hand, being able to ssh into the nodes and poke around is a big plus for a Kubernetes beginner such as myself.

# Install k3s

To install [k3s][] I mainly referenced [this video][k3s-deploy-video] from [Jim's Garage][] and he's got a [k3s deploy script][] that I started out with.

::: warning
While I modified and ran the script some of the commands failed, giving me all sorts of headache.
It was partly due to me running [Void Linux][] instead of Ubuntu on my desktop and partly because
my nodes weren't [prepared properly][].

I think it might be wise to run the commands manually to make sure they execute correctly.
:::

[k3s-deploy-video]: https://www.youtube.com/watch?v=6k8BABDXeZI

## Install [k3sup][] and kubectl

We're going to need `kubectl` and [k3sup][] on the machine we're going to install our cluster from (meaning, my computer).

Install [k3sup][]:

```bash
curl -sLS https://get.k3sup.dev | sh
sudo install k3sup /usr/local/bin/
```

```fish-shell
$ k3sup version
 _    _____
| | _|___ / ___ _   _ _ __
| |/ / |_ \/ __| | | | '_ \
|   < ___) \__ \ |_| | |_) |
|_|\_\____/|___/\__,_| .__/
                     |_|

bootstrap K3s over SSH in < 60s üöÄ
üëè Say thanks for k3sup and sponsor Alex via GitHub: https://github.com/sponsors/alexellis

Version: 0.13.9
Git Commit: a1700f64dcffd249890b13cf6d97f4c120a53e08
```

I installed `kubectl` via the package manager on Void Linux:

```bash
sudo xbps-install -Suy kubectl
```

```fish-shell
$ kubectl version
Client Version: v1.33.1
Kustomize Version: v5.6.0
Error from server (NotFound): the server could not find the requested resource
```

## Install policycoreutils

The script installs `policycoreutils` on each node (although to be honest, I didn't try it without so I don't know if it's truly necessary).

```bash
NEEDRESTART_MODE=a apt-get install policycoreutils -y
```

[k3sup]: https://github.com/alexellis/k3sup

## Boostrap the first master node

```bash
# K3S Version
k3sVersion="v1.33.1+k3s1"

# IP addresse of the node
master1=10.1.2.2

# Set the virtual IP address (VIP)
vip=10.1.2.1

# Interface used on remotes
interface=eth0

# User of remote machines
user=tree

# Do the installation
mkdir ~/.kube
k3sup install \
  --ip $master1 \
  --user $user \
  --tls-san $vip \
  --cluster \
  --k3s-version $k3sVersion \
  --k3s-extra-args "--disable traefik --disable servicelb --flannel-iface=$interface --node-ip=$master1" \
  --merge \
  --sudo \
  --local-path $HOME/.kube/config \
  --context k3s-ha
```

A couple of things to note here:

* We're going to setup a high availability cluster and we're later going to setup [kube-vip][] to provide a virtual IP address for the whole cluster (the `$vip`{=bash} variable).
* For [k3sup][] to work the user must be able to run passwordless `sudo` commands (or run as root user, then we don't need the `--sudo` parameter).
* I used the latest version of [k3s][]: `v1.33.1+k3s1` but that gave me some problems installing Rancher with helm. A slightly older version might have been better.
* We disable [Traefik][] as we'll set that up later and we'll disable ServiceLB too as we'll use [kube-vip][] and [metallb][] for load balancing.
* The `~/.kube/config` file will be used to authenticate to the cluster.
  When the above command failed for me I had to remove it before I could run it again.

It should hopefully finish okay, which you can verify with `kubectl`:

```fish-shell
$ kubectl get nodes
NAME        STATUS   ROLES                       AGE   VERSION
kube-pi5a   Ready    control-plane,etcd,master   23h   v1.33.1+k3s1
```

## Install [kube-vip][]

With our first Kubernetes node up and running we can add [kube-vip][] as a service on the cluster itself, which will provide the virtual IP address for cluster itself.
This is neat as we don't have to have any extra hardware or software "in front" of the cluster to act as an entry point.

This way the cluster can use a single IP address even when some of the nodes shuts down.
It feels a bit magical to me but people smarter than me have found a way.

First create the RBAC settings (permissions):

```bash
kubectl apply -f https://kube-vip.io/manifests/rbac.yaml
```

We need need to create a beautiful yaml file at `/var/lib/rancher/k3s/server/manifests/kube-vip.yaml` on our master node.
[kube-vip][] has an [example of how to generate it][] but I simply used Jim's file:

```bash
curl -sO https://raw.githubusercontent.com/JamesTurland/JimsGarage/main/Kubernetes/K3S-Deploy/kube-vip
cat kube-vip | sed 's/$interface/'$interface'/g; s/$vip/'$vip'/g' > $HOME/kube-vip.yaml
```

You can verify the file afterwards but the main difference is that it pins [kube-vip][] to version `v0.8.2` and sets up our interface (`eth0`) and virtual IP (`10.1.2.1`) that we set previously.

## Setup the rest of the nodes

We can now setup the rest of the nodes and join them to our master node:

```bash
# The other two master nodes
master2=10.1.2.3
master3=10.1.2.4
masters=($master2 $master3)

for newnode in "${masters[@]}"; do
  k3sup join \
    --ip $newnode \
    --user $user \
    --sudo \
    --k3s-version $k3sVersion \
    --server \
    --server-ip $master1 \
    --k3s-extra-args "--disable traefik --disable servicelb --flannel-iface=$interface --node-ip=$newnode" \
    --server-user $user
done
```

Hopefully everything completes as it should and we should now have 3 nodes:

```fish-shell
$ kubectl get nodes
NAME        STATUS   ROLES                       AGE   VERSION
kube-pi5a   Ready    control-plane,etcd,master   23h   v1.33.1+k3s1
kube-pi5b   Ready    control-plane,etcd,master   23h   v1.33.1+k3s1
kube-pi5c   Ready    control-plane,etcd,master   23h   v1.33.1+k3s1
```

Adding workers is similar:

```bash
# The worker IPs
worker1=10.1.2.5
worker2=10.1.2.6

for newagent in "${workers[@]}"; do
  k3sup join \
    --ip $newagent \
    --user $user \
    --sudo \
    --k3s-version $k3sVersion \
    --server-ip $master1
done
```

```fish-shell
$ kubectl get nodes
NAME        STATUS   ROLES                       AGE   VERSION
kube-pi5a   Ready    control-plane,etcd,master   24h   v1.33.1+k3s1
kube-pi5b   Ready    control-plane,etcd,master   23h   v1.33.1+k3s1
kube-pi5c   Ready    control-plane,etcd,master   23h   v1.33.1+k3s1
kube-pi5d   Ready    <none>                      23h   v1.33.1+k3s1
kube-pi5e   Ready    <none>                      23h   v1.33.1+k3s1
```

::: note
I do have six Raspberry Pis I want to create my cluster with.
However, I hit a small snag and had to use one of the NVMe disks for my Proxmox server so I did the initial setup using 5 Pis instead of 6.
It's fine as you can easily add more workers with the above command at a later time.
:::

## Setup load balancer

Finally, we'll setup a load balancer.
You should be able to use [kube-vip][] as a load balancer too but Jim's script installs [metallb][] and it seems like a popular solution so why not?

::: note
Jim's script now installs the [kube-vip cloud provider][]:

```bash
kubectl apply -f https://raw.githubusercontent.com/kube-vip/kube-vip-cloud-provider/main/manifest/kube-vip-cloud-controller.yaml
```

But I'm not sure why as it uses [metallb][] for load balancing.
Is it an oversight or am I missing something?
:::

Let's install [metallb][]:

```bash
kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.12.1/manifests/namespace.yaml
kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.13.12/config/manifests/metallb-native.yaml
```

And setup an IP range for the load balancer to use:

{path="ipAddressPool"}
```yaml
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: first-pool
  namespace: metallb-system
spec:
  addresses:
  - 10.1.2.100-10.1.2.250
```

```bash
kubectl apply -f ipAddressPool
```

And we should be done!
Can we verify it?

```fish-shell
$ kubectl get pods -n metallb-system
NAME                       READY   STATUS    RESTARTS   AGE
controller-c76b688-dntdl   1/1     Running   0          36h
speaker-7zf8t              1/1     Running   0          36h
speaker-l82mt              1/1     Running   0          36h
speaker-q2876              1/1     Running   0          36h
speaker-sbc4g              1/1     Running   0          36h
speaker-x4cxt              1/1     Running   0          36h
```

Something is running. I guess that's a good thing.

## Test by deploying nginx

To make sure things are working correctly we can try to start a service and see if we can access it.
Jim's script uses nginx and exposes it via the load balancer like so:

```bash
kubectl apply -f https://raw.githubusercontent.com/inlets/inlets-operator/master/contrib/nginx-sample-deployment.yaml -n default
kubectl expose deployment nginx-1 --port=80 --type=LoadBalancer -n default
```

After a while you should be able to see the pod running:

```fish-shell
$ kubectl get pods
NAME                       READY   STATUS    RESTARTS   AGE
nginx-1-647677fc66-2jzj6   1/1     Running   0          23h
```

```fish-shell
$ kubectl get svc
NAME         TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
kubernetes   ClusterIP      10.43.0.1      <none>        443/TCP        24h
nginx-1      LoadBalancer   10.43.20.159   10.1.2.100    80:31508/TCP   23h
```

Here we see that nginx has been assigned the external IP `10.1.2.100`, which is the first IP address in the loadbalancer range.
If I visit it in the browser I'm greeted with the "Welcome to nginx!" message and I have my very first service operational.
Yay!

# Rancher to manage Kubernetes

![Rancher offers a view of the nodes and their usage, among a bunch of other things.](/images/kubernetes/rancher_nodes.png)

One thing I've been missing with my old homelab setup where I `ssh` to my server and run containers via `docker compose` is a nice dashboard.
As I'm already using [k3s][] and plan on using [Longhorn][], it makes sense to try out [Rancher][] as it's made by the same team.

By complete coincidence there's also a [Jim's Garage][] video about [deploying Rancher][] and an [accompanying readme][] with instructions on how to install it.

## Install helm

I'll install [Rancher][] using [Helm][], so [let's install it][]:

```bash
curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
chmod 700 get_helm.sh
./get_helm.sh
```

```fish-shell
$ helm version
version.BuildInfo{Version:"v3.18.2", GitCommit:"04cad4610054e5d546aa5c5d9c1b1d5cf68ec1f8", GitTreeState:"clean", GoVersion:"go1.24.3"}
```

## Install cert-manager

Next, we'll [install cert-manager][]:

```bash
kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.18.0/cert-manager.crds.yaml
helm repo add jetstack https://charts.jetstack.io
helm repo update
helm install cert-manager jetstack/cert-manager \
   --namespace cert-manager \
   --create-namespace \
   --version v1.18.0
```

And verify installation:

```fish-shell
$ kubectl get pods --namespace cert-manager
NAME                                       READY   STATUS    RESTARTS   AGE
cert-manager-788d58b76f-7wws8              1/1     Running   0          2d
cert-manager-cainjector-5f6f659459-znprj   1/1     Running   0          2d
cert-manager-webhook-75d4c8db8b-97djk      1/1     Running   0          2d
```

## Install Rancher

Now we can finally install [Rancher][].
First, the repo:

```bash
helm repo add rancher-latest https://releases.rancher.com/server-charts/latest
```

It's apparently good style to use a namespace:

```bash
kubectl create namespace cattle-system
```

The next step should be to install it, making sure to use the domain you want to use for your homelab:

```bash
helm install rancher rancher-latest/rancher \
   --namespace cattle-system \
   --set hostname=rancher.hietala.xyz \
   --set bootstrapPassword=admin
```

However,
I got a version incompatibility error:

```
Error: INSTALLATION FAILED: chart requires kubeVersion: < 1.33.0-0 which is incompatible with Kubernetes v1.33.1+k3s1
```

This didn't go away even if I used the alpha version of [Rancher][] with `https://releases.rancher.com/server-charts/alpha` or updated [Helm][] to the latest version.

One solution would be to downgrade Kubernetes to an older version...
But that sounds annoying.

I instead
[found a workaround](https://github.com/rancher/rancher/issues/43092#issuecomment-2423135070)
that removes the `kubeVersion` requirement from the [Rancher][] chart and then installs it:

```bash
mkdir tmp
cd tmp
helm fetch rancher-latest/rancher --untar
sed -i "/kubeVersion/d" Chart.yaml
helm install rancher . \
   --namespace cattle-system \
   --set hostname=rancher.hietala.xyz \
   --set bootstrapPassword=admin
```

Yeah, it may explode in your face but it seems to have worked this time.

This can take a while so we can wait and confirm the deployment with:

```bash
kubectl -n cattle-system rollout status deploy/rancher
kubectl -n cattle-system get deploy rancher
```

## Expose Rancher via load balancer

But how do we access [Rancher][]?

```fish-shell
$ kubectl get svc -n cattle-system
NAME                       TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
imperative-api-extension   ClusterIP      10.43.134.212   <none>        6666/TCP         2d
rancher                    ClusterIP      10.43.151.31    <none>        80/TCP,443/TCP   2d
rancher-webhook            ClusterIP      10.43.38.150    <none>        443/TCP          2d
```

There's no external IP we can access?

Like we did [with nginx last time](#Test-by-deploying-nginx) we need to expose the service via the load balancer:

```bash
kubectl expose deployment rancher --name=rancher-lb --port=443 --type=LoadBalancer -n cattle-system
```

This time we exposed a new service on port `443` so it secures the connection and it also generates a self-signed certificate for us.

See:

{hl=5}
```fish-shell
$ kubectl get svc -n cattle-system
NAME                       TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
imperative-api-extension   ClusterIP      10.43.134.212   <none>        6666/TCP         2d
rancher                    ClusterIP      10.43.151.31    <none>        80/TCP,443/TCP   2d
rancher-lb                 LoadBalancer   10.43.118.11    10.1.2.101    443:31413/TCP    2d
rancher-webhook            ClusterIP      10.43.38.150    <none>        443/TCP          2d
```

Excellent.

# Storage

## Setup local-path storage

<https://github.com/rancher/local-path-provisioner>

Used for highly performance sensitive services such as databases\
Services that rely on SQLite might also need to run there?

- Postgres
- Influxdb

## Install Longhorn

1. install some packages such as the [iscsi_tcp module][] ?

<https://harrytang.xyz/blog/how-to-longhorn-k8s>

<https://www.youtube.com/watch?v=ps0NKd59UkE>
<https://github.com/JamesTurland/JimsGarage/blob/main/Kubernetes/Longhorn/longhorn-K3S.sh>

Dependencies and stuff:

```
sudo apt install
    python3-pip
    git
    apt-transport-https
    curl
    avahi-daemon
    nfs-common
    linux-modules-extra-raspi

```

Used for everything else where performance isn't critical.
Excellent because it's replicated.

# Setup services

1. `mqtt`:Longhorn
1. `Actualbudget`: Longhorn

## Home automation setup

- Postgres
- haex
- Home Assistant
- mqtt
- zigbee2mqtt
- Music Assistant

## Other services

- Grafana
- influxdb
- unifi controller
- freshrss
- reddit-top-rss
- actualbudget
- newt (expose to pangolin)
- tailscale
- traefik / ingress controller

## Lannisport worker

- Jellyfin
- Stash

# Node setup

Raspberry Pi 5:

- Control + Longhorn
- Control + Longhorn
- Control + Longhorn
- Worker + Longhorn
- Worker + local-path (with Zigbee dongle)
- Worker + local-path

Raspberry Pi 4 (maybe):

- Worker

Lannsport proxmox:

- Worker with GPU + local-path
- Extra Longhorn deployment

Where to have Zigbee dongle?
Maybe just pick one Pi and set it up?

[iscsi_tcp module]: https://askubuntu.com/questions/1373309/missing-iscsi-tcp-kernel-module-in-ubuntu-21-10-for-raspberry-pi-arm64
[PNY CS2230 M.2 NVMe SSD 1TB]: https://www.inet.se/produkt/4305547/pny-cs2230-m-2-nvme-ssd-1tb
[Raspberry Pi OS]: https://www.raspberrypi.com/software/
[Server Mark III for Raspbery Pi 5]: https://www.printables.com/model/685991-raspberry-pi-server-mark-iii-for-raspbery-pi-5
[MINI RACK]: https://mini-rack.jeffgeerling.com/
[about the MINI RACK]: https://www.youtube.com/watch?v=y1GCIwLm3is
[3d-printed-rack]: https://www.printables.com/model/1170708-modular-1010-inch-rack
[OPNsense]: https://opnsense.org/
[Void Linux]: https://voidlinux.org/
[Ubuntu server]: https://ubuntu.com/download/server
[Talos Linux]: https://www.talos.dev/
[RKE2]: https://docs.rke2.io/
[minikube]: https://minikube.sigs.k8s.io/docs/
[k3s]: https://minikube.sigs.k8s.io/docs/
[Jim's Garage]: https://www.youtube.com/@Jims-Garage
[k3s deploy script]: https://github.com/JamesTurland/JimsGarage/tree/main/Kubernetes/K3S-Deploy
[prepared properly]: /drafts/kubernetes/#Prepare-for-Kubernetes
[kube-vip]: https://kube-vip.io/
[example of how to generate it]: https://kube-vip.io/docs/installation/daemonset/#arp-example-for-daemonset

With our first Kubernetes node up and running we can add kube-vip as a service on the cluster itself, which will provide the virtual IP address for cluster itself. This is neat as we don‚Äôt have to have any extra hardware or software ‚Äúin front‚Äù of the cluster to act as an entry point.

This way the cluster can use a single IP address even when some of the nodes shuts down. It feels a bit magical to me but people smarter than me have found a way.

First create the RBAC settings (permissions):

kubectl apply -f https://kube-vip.io/manifests/rbac.yaml

We need need to create a beautiful yaml file at /var

[kube-vip cloud provider]: https://github.com/kube-vip/kube-vip-cloud-provider
[metallb]: https://metallb.io/
[Traefik]: https://traefik.io/traefik/
[deploying Rancher]: https://www.youtube.com/watch?v=hT2_O2Yd_wE
[accompanying readme]: https://github.com/JamesTurland/JimsGarage/tree/main/Kubernetes/Rancher-Deployment
[Longhorn]: https://longhorn.io/
[Rancher]: https://ranchermanager.docs.rancher.com/v2.6
[let's install it]: https://helm.sh/docs/intro/install/
[Helm]: https://helm.sh/
[install cert-manager]: https://cert-manager.io/docs/installation/helm/
